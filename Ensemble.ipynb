{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"./dataset/heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode categorical features\n",
    "label_encoder = LabelEncoder()\n",
    "categorical_cols = ['Sex', 'ChestPainType', 'FastingBS', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "for col in categorical_cols:\n",
    "    df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df.drop('HeartDisease', axis=1)\n",
    "y = df['HeartDisease']\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained models\n",
    "ann_model = load_model('./models/ANN.h5')\n",
    "gru_model = load_model('./models/GRU.h5')\n",
    "lstm_model = load_model('./models/LSTM.h5')\n",
    "mlp_model = load_model('./models/MLP.h5')\n",
    "rnn_model = load_model('./models/SimpleRNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 1s 2ms/step\n",
      "29/29 [==============================] - 0s 2ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n",
      "29/29 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict probabilities\n",
    "y_pred_probs_ann = ann_model.predict(X)\n",
    "y_pred_probs_gru = gru_model.predict(X[:, None, :])\n",
    "y_pred_probs_lstm = lstm_model.predict(X[:, None, :])\n",
    "y_pred_probs_mlp = mlp_model.predict(X)\n",
    "y_pred_probs_rnn = rnn_model.predict(X[:, None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert probabilities to class predictions\n",
    "y_pred_ann = np.round(y_pred_probs_ann).astype(int)\n",
    "y_pred_gru = np.round(y_pred_probs_gru).astype(int)\n",
    "y_pred_lstm = np.round(y_pred_probs_lstm).astype(int)\n",
    "y_pred_mlp = np.argmax(y_pred_probs_mlp, axis=-1)\n",
    "y_pred_rnn = np.round(y_pred_probs_rnn).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure predictions are in the same shape\n",
    "y_pred_ann = y_pred_ann.flatten()\n",
    "y_pred_gru = y_pred_gru.flatten()\n",
    "y_pred_lstm = y_pred_lstm.flatten()\n",
    "y_pred_mlp = y_pred_mlp.flatten()\n",
    "y_pred_rnn = y_pred_rnn.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement voting system using mode\n",
    "y_pred_combined_mode = np.array([y_pred_ann, y_pred_gru, y_pred_lstm, y_pred_mlp, y_pred_rnn])\n",
    "y_pred_ensemble_mode = mode(y_pred_combined_mode, axis=0)[0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ensemble model\n",
    "accuracy = accuracy_score(y, y_pred_ensemble_mode)\n",
    "precision = precision_score(y, y_pred_ensemble_mode)\n",
    "recall = recall_score(y, y_pred_ensemble_mode)\n",
    "f1 = f1_score(y, y_pred_ensemble_mode)\n",
    "roc_auc = roc_auc_score(y, y_pred_ensemble_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9379084967320261\n",
      "Precision: 0.9430255402750491\n",
      "Recall: 0.9448818897637795\n",
      "F1 Score: 0.943952802359882\n",
      "ROC AUC Score: 0.9370750912233533\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"ROC AUC Score: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement mean ensemble\n",
    "y_pred_combined_mean = np.array([y_pred_ann, y_pred_gru, y_pred_lstm, y_pred_mlp, y_pred_rnn])\n",
    "y_pred_ensemble_mean = np.round(np.mean(y_pred_combined_mean, axis=0)).astype(int)\n",
    "\n",
    "\n",
    "# For classification tasks, convert the mean probabilities to class labels\n",
    "y_pred_ensemble_mean_class = np.round(y_pred_ensemble_mean).astype(int)\n",
    "y_pred_ensemble_mean_class= y_pred_ensemble_mean_class.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate ensemble model\n",
    "accuracy = accuracy_score(y, y_pred_ensemble_mean_class)\n",
    "precision = precision_score(y, y_pred_ensemble_mean_class, average='weighted')\n",
    "recall = recall_score(y, y_pred_ensemble_mean_class,average='weighted')\n",
    "f1 = f1_score(y, y_pred_ensemble_mean_class,average='weighted')\n",
    "roc_auc = roc_auc_score(y, y_pred_ensemble_mean_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9379084967320261\n",
      "Precision: 0.937895985623119\n",
      "Recall: 0.9379084967320261\n",
      "F1 Score: 0.9379011166274744\n",
      "ROC AUC Score: 0.9370750912233533\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"ROC AUC Score: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN Accuracy: 0.9400871459694989\n"
     ]
    }
   ],
   "source": [
    "ann_accuracy = accuracy_score(y, y_pred_ann)\n",
    "print(\"ANN Accuracy:\", ann_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU Accuracy: 0.9215686274509803\n"
     ]
    }
   ],
   "source": [
    "gru_accuracy = accuracy_score(y, y_pred_gru)\n",
    "print(\"GRU Accuracy:\", gru_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Accuracy: 0.9248366013071896\n"
     ]
    }
   ],
   "source": [
    "lstm_accuracy = accuracy_score(y, y_pred_lstm)\n",
    "print(\"LSTM Accuracy:\", lstm_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP Accuracy: 0.9237472766884531\n"
     ]
    }
   ],
   "source": [
    "mlp_accuracy = accuracy_score(y, y_pred_mlp)\n",
    "print(\"MLP Accuracy:\", mlp_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Accuracy: 0.9411764705882353\n"
     ]
    }
   ],
   "source": [
    "rnn_accuracy = accuracy_score(y, y_pred_rnn)\n",
    "print(\"RNN Accuracy:\", rnn_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
